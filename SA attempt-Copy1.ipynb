{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: to do\n",
    "- Embeddings\n",
    "- Lexicons\n",
    "- Run demo\n",
    "- Train classifiers\n",
    "- Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import re\n",
    "import statsmodels.formula.api\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure how graphs will show up in this notebook\n",
    "%matplotlib inline\n",
    "seaborn.set_context('notebook', rc={'figure.figsize': (10, 6)}, font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1917494, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import word embeddings\n",
    "def load_embeddings(filename):\n",
    "    labels = []\n",
    "    rows = []\n",
    "    with open(filename, encoding='utf-8') as infile:\n",
    "        for i, line in enumerate(infile):\n",
    "            items = line.rstrip().split(' ')\n",
    "            if len(items) == 2:\n",
    "                # This is a header row giving the shape of the matrix\n",
    "                continue\n",
    "            labels.append(items[0])\n",
    "            values = np.array([float(x) for x in items[1:]], 'f')\n",
    "            rows.append(values)\n",
    "    \n",
    "    arr = np.vstack(rows)\n",
    "    return pd.DataFrame(arr, index=labels, dtype='f')\n",
    "\n",
    "\n",
    "# Call the method\n",
    "embeddings = load_embeddings('glove.42B.300d.txt')\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"state-of-the-art\" Lexicon: https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon\n",
    "def load_lexicon(filename):\n",
    "    lexicon = []\n",
    "    with open(filename, encoding='latin-1') as infile:\n",
    "        for line in infile:\n",
    "            line = line.rstrip()\n",
    "            if line and not line.startswith(';'):\n",
    "                lexicon.append(line)\n",
    "    return lexicon\n",
    "\n",
    "\n",
    "# Call the method\n",
    "pos_words = load_lexicon('positive-words.txt')\n",
    "neg_words = load_lexicon('negative-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarising...\n",
      "Splitting...\n"
     ]
    }
   ],
   "source": [
    "# Deal with mismatches between the embedding and the lexicon that create NaN values\n",
    "pos_vectors = embeddings.loc[pos_words].dropna()\n",
    "neg_vectors = embeddings.loc[neg_words].dropna()\n",
    "\n",
    "# Match sentiment to polarity\n",
    "print(\"Polarising...\")\n",
    "vectors = pd.concat([pos_vectors, neg_vectors])\n",
    "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
    "labels = list(pos_vectors.index) + list(neg_vectors.index)\n",
    "\n",
    "print(\"Splitting...\")\n",
    "# Split data\n",
    "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
    "    train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make our classifier, and train it by running the training vectors through it for 100 iterations. We use a logistic function as the loss, so that the resulting classifier can output the probability that a word is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.947209653092006"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification time!\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgClassifier = LogisticRegression(solver='lbfgs')\n",
    "lgClassifier.fit(train_vectors, train_targets)\n",
    "\n",
    "# Score for out-of-training data\n",
    "accuracy_score(lgClassifier.predict(test_vectors), test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9502262443438914"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification time!\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "lgClassifier = MultinomialNB() \n",
    "lgClassifier.fit(train_vectors, train_targets)\n",
    "\n",
    "# Score for out-of-training data\n",
    "accuracy_score(lgClassifier.predict(test_vectors), test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_log_proba gives the log probability for each class\n",
    "def vecs_to_sentiment(vecs):\n",
    "    \n",
    "    predictions = lgClassifier.predict_log_proba(vecs)\n",
    "\n",
    "    # To see an overall positive vs. negative classification in one number,\n",
    "    # we take the log probability of positive sentiment minus the log\n",
    "    # probability of negative sentiment.\n",
    "    return predictions[:, 1] - predictions[:, 0]\n",
    "\n",
    "\n",
    "# log-prob for specific words or sentences\n",
    "def words_to_sentiment(words):\n",
    "    vecs = embeddings.loc[words].dropna()\n",
    "    log_odds = vecs_to_sentiment(vecs)\n",
    "    return pd.DataFrame({'sentiment': log_odds}, index=vecs.index)\n",
    "\n",
    "\n",
    "# The regex above finds tokens that start with a word-like character (\\w), and continues\n",
    "# matching characters (.+?) until the next word break (\\b). It's a relatively simple\n",
    "# expression that manages to extract something very much like words from text.\n",
    "TOKEN_RE = re.compile(r\"\\w.*?\\b\")\n",
    "\n",
    "# Combine sentiments for word vectors into an overall sentiment scoreby averaging them.\n",
    "def text_to_sentiment(text):\n",
    "    tokens = [token.casefold() for token in TOKEN_RE.findall(text)]\n",
    "    sentiments = words_to_sentiment(tokens)\n",
    "    return sentiments['sentiment'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Other approaches\n",
    "\n",
    "This is of course only one way to do sentiment analysis. All the steps we used are common, but you probably object that you wouldn't do it that way. But if you have your own process, I urge you to see if your process is encoding prejudices and biases in the model it learns.\n",
    "\n",
    "Instead of or in addition to changing your source of word vectors, you could try to fix this problem in the output directly. It may help, for example, to build a stronger model of whether sentiment should be assigned to words at all, designed to specifically exclude names and groups of people.\n",
    "\n",
    "You could abandon the idea of inferring sentiment for words, and only count the sentiment of words that appear exactly in the list. This is perhaps the most common form of sentiment analysis -- the kind that includes no machine learning at all. Its results will be no more biased than whoever made the list. But the lack of machine learning means that this approach has low recall, and the only way to adapt it to your data set is to edit the list manually.\n",
    "\n",
    "As a hybrid approach, you could produce a large number of inferred sentiments for words, and have a human annotator patiently look through them, making a list of exceptions whose sentiment should be set to 0. The downside of this is that it's extra work; the upside is that you take the time to actually see what your data is doing. And that's something that I think should happen more often in machine learning anyway.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6016286987257962"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_sentiment(\"My name is Ahmed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1572904\n",
      "809290\n",
      "763614\n",
      "1572904\n"
     ]
    }
   ],
   "source": [
    "# Importing data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('FINAL-OUTPUT.txt') as f:\n",
    "    full_data = [line.rstrip('\\n') for line in f]\n",
    "with open('MEN-OUTPUT.txt') as f:\n",
    "    man_data = [line.rstrip('\\n') for line in f]\n",
    "with open('WOMEN-OUTPUT.txt') as f:\n",
    "    woman_data = [line.rstrip('\\n') for line in f]\n",
    "print(len(full_data))\n",
    "print(len(man_data))\n",
    "print(len(woman_data))\n",
    "print(len(woman_data) + len(man_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organise data\n",
    "import json\n",
    "\n",
    "gender = []\n",
    "body = []\n",
    "subreddit = []\n",
    "for line in full_data:\n",
    "    temp = json.loads(line)\n",
    "    body.append(temp['body'])\n",
    "    \n",
    "    if (temp['author_flair_css_class'] == 'male'):\n",
    "        gender.append(1)\n",
    "    else:\n",
    "        gender.append(0)\n",
    "        \n",
    "    if (temp['subreddit'] == 'AskMen'):\n",
    "        subreddit.append('AM')\n",
    "    else:\n",
    "        subreddit.append('AW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "sentimentOfAM = []\n",
    "sentimentOfAW = []\n",
    "sentimentOfMen = []\n",
    "sentimentOfWomen = []\n",
    "\n",
    "for i, item in enumerate(body):\n",
    "    try:\n",
    "        sentiment.append(text_to_sentiment(item))  \n",
    "        \n",
    "        if subreddit[i] == 'AM':\n",
    "            sentimentOfAM.append(sentiment[i])\n",
    "        else:\n",
    "            sentimentOfAW.append(sentiment[i])\n",
    "            \n",
    "        if gender[i] == 1:\n",
    "            sentimentOfMen.append(sentiment[i])\n",
    "        else:\n",
    "            sentimentOfWomen.append(sentiment[i])\n",
    "    except:\n",
    "        sentiment.append(0)\n",
    "        #print(\"offie owwie I appended a fake numberie at\", len(sentiment)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1572904\n",
      "808375\n",
      "762763\n",
      "862456\n",
      "708682\n"
     ]
    }
   ],
   "source": [
    "print(len(sentiment))\n",
    "print(len(sentimentOfAM))\n",
    "print(len(sentimentOfAW))\n",
    "print(len(sentimentOfMen))\n",
    "print(len(sentimentOfWomen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentiment))\n",
    "print(len(sentimentOfAM))\n",
    "print(len(sentimentOfAW))\n",
    "print(len(sentimentOfMen))\n",
    "print(len(sentimentOfWomen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General: 0.9796917245649305\n",
      "AskMenSent: 0.9593854818054988\n",
      "AskWomenSent: 1.0034804958820738\n",
      "MenSent: 0.9566705501064069\n",
      "WomenSent: 1.0101495118719404\n"
     ]
    }
   ],
   "source": [
    "general = sum(sentiment)/len(sentiment)\n",
    "print(\"General:\", general)\n",
    "\n",
    "AskMenSent = sum(sentimentOfAM)/len(sentimentOfAM)\n",
    "print(\"AskMenSent:\", AskMenSent)\n",
    "\n",
    "AskWomenSent = sum(sentimentOfAW)/len(sentimentOfAW)\n",
    "print(\"AskWomenSent:\", AskWomenSent)\n",
    "\n",
    "MenSent = sum(sentimentOfMen)/len(sentimentOfMen)\n",
    "print(\"MenSent:\", MenSent)\n",
    "\n",
    "WomenSent = sum(sentimentOfWomen)/len(sentimentOfWomen)\n",
    "print(\"WomenSent:\", WomenSent)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
